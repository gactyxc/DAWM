{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"mount_file_id":"1-U3Rjzu9KiQLv_LulmjMxB1BnRX-hkRe","authorship_tag":"ABX9TyNMVttoYRyqetlxzNoFprBs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. data pre-processing (GEE)"],"metadata":{"id":"u_Alp7B4b4kP"}},{"cell_type":"markdown","source":["## initialize the gee"],"metadata":{"id":"7gvhKss0b9DA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4uzslKRbnBb"},"outputs":[],"source":["! pip install earthengine-api\n","import ee\n","ee.Authenticate()\n","ee.Initialize(project='crops-mapping-gaoyuan')\n","from ee.batch import Export\n","!pip install geemap\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import time"]},{"cell_type":"markdown","source":["## image process related function"],"metadata":{"id":"ixn77Y1mcJTs"}},{"cell_type":"code","source":["# common functions\n","# function to get the sentinel-2 image collection based on the study data range and study area ----*/\n","# function to remove cloud\n","# function to exclude bad data at scene edges\n","def maskEdges(s2_img):\n","    return s2_img.updateMask(\n","        s2_img.select('B8A').mask().updateMask(s2_img.select('B9').mask()))\n","\n","# Function to mask clouds in Sentinel-2 imagery.\n","def maskClouds(img):\n","    max_cloud_probabiltly = 5\n","    clouds = ee.Image(img.get('cloud_mask')).select('probability')\n","    isNotCloud = clouds.lt(max_cloud_probabiltly)\n","    return img.updateMask(isNotCloud)\n","\n","def sentinel2_collection(start_data, end_data, roi):\n","    s2Sr = ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\")\n","    s2Clouds = ee.ImageCollection(\"COPERNICUS/S2_CLOUD_PROBABILITY\")\n","\n","    # define the filter constraints\n","    criteria = ee.Filter.And(ee.Filter.geometry(roi), ee.Filter.date(start_data, end_data))\n","\n","    # sentinel-2 data collection\n","    sentinel2_bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n","    new_bands = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n","\n","    # Filter input collections by desired data range and region.\n","    s2Sr = s2Sr.filter(criteria).map(maskEdges)\n","    s2Clouds = s2Clouds.filter(criteria)\n","\n","    # Join S2 SR with cloud probability dataset to add cloud mask.\n","    s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(**{\n","      \"primary\": s2Sr,\n","      \"secondary\": s2Clouds,\n","      \"condition\": ee.Filter.equals(**{\"leftField\": \"system:index\", \"rightField\":\"system:index\"})\n","      })\n","\n","    # collect the images without cloud\n","    s2CloudMasked = ee.ImageCollection(s2SrWithCloudMask).map(maskClouds).select(sentinel2_bands, new_bands)\n","    return s2CloudMasked\n","# add EVI\n","def addEVI(image):\n","    EVI = image.expression('EVI = 2500 * (NIR - R) / (NIR + 6 * R - 7.5 * B + 10000)', {\n","        'NIR': image.select('NIR'),\n","        'R': image.select('R'),\n","        'B': image.select('B')\n","    })\n","    return image.addBands(EVI)\n","\n","# year: the target identification year\n","# startDoy: the start Doy of optimal identification window\n","# endDoy: the end Doy of optimal identification window\n","# roi: target region of study\n","# output value of this function is the time series s2sr images in a given time period\n","def get_s2sr_images(start_date, end_date,interval, roi):\n","\n","    # Define the year of the start_date\n","    year = ee.Date(start_date).get('year')\n","    first_date = ee.Date.fromYMD(year, 1, 1)\n","    last_date = ee.Date.fromYMD(year.add(1), 12, 31)\n","\n","    # Define the Sentinel-2 image collection\n","    s2SR_imgCol = sentinel2_collection(first_date, last_date, roi)\n","\n","    # Create a date range list with a specified interval\n","    millis_interval = ee.Number(interval).multiply(1000 * 60 * 60 * 24)\n","    dates = ee.List.sequence(start_date.millis(), end_date.millis(), millis_interval)\n","\n","    # function to resample time resolution of image collection to 10 day\n","    def resampleTo10Days(date):\n","        currentDate = ee.Date(date)\n","        endDate = currentDate.advance(interval, 'day')\n","        summarizedImageCol = s2SR_imgCol.filterDate(currentDate, endDate)\n","        summarizedImage = summarizedImageCol.median()\n","        summarizedImage = summarizedImage.set('system:time_start', date)\n","        return summarizedImage\n","\n","    # Apply the time resampling function using map()\n","    resampledImages = ee.ImageCollection(dates.map(resampleTo10Days))\n","    return resampledImages"],"metadata":{"id":"HQaS9gKtcKJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_random_samples(roi):\n","    ESA_landmap = ee.ImageCollection(\"ESA/WorldCover/v100\").first()\n","    cropland_WithMask = ESA_landmap.updateMask(ESA_landmap.eq(40))\n","    random_samples = cropland_WithMask.sample(\n","        region=roi,\n","        scale=10,\n","        numPixels=10000,\n","        geometries=True\n","    )\n","    indices = ee.List.sequence(0, random_samples.size().subtract(1))\n","    points_with_fid = ee.FeatureCollection(\n","        random_samples.toList(random_samples.size())\n","        .zip(indices)\n","        .map(lambda feature_and_index: ee.Feature(\n","            ee.List(feature_and_index).get(0)\n","        ).set('FID', ee.Number(ee.List(feature_and_index).get(1))))\n","    )\n","    points_with_coordinates = points_with_fid.map(lambda feature: feature.set({\n","        'longitude': feature.geometry().coordinates().get(0),\n","        'latitude': feature.geometry().coordinates().get(1)\n","    }))\n","    return points_with_coordinates\n","\n","def extract_points_value(imgCol, pts,fileName,folderName):\n","  ft = ee.FeatureCollection(ee.List([]))\n","\n","  def fill(img, ini):\n","    date = ee.Date(img.date()).format()\n","    inift = ee.FeatureCollection(ini)\n","    ft2 = img.sampleRegions(\n","        collection = pts,\n","        properties = ['FID','longitude','latitude','Label'], # Properties to include from points\n","        scale = 10\n","    )\n","    ft3 = ft2.map(lambda f: f.set('date', date))\n","    return inift.merge(ft3)\n","  newft = ee.FeatureCollection(imgCol.iterate(fill, ft))\n","  task = ee.batch.Export.table.toDrive(\n","      collection = newft,\n","      description = fileName,\n","      folder = folderName,\n","      fileFormat = 'CSV'\n","  )\n","  task.start()"],"metadata":{"id":"RaWA3kR2cNo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_attributes_ofSamples(year, interval, roi,tilename):\n","    folderName = 'test_' + str(tilename)\n","    filename = 'primary_' + str(tilename) + '_' + str(year)\n","    startDate = ee.Date.fromYMD(year-1, 10, 1)\n","    endDate = ee.Date.fromYMD(year, 10, 1)\n","    s2SR_imgCol = get_s2sr_images(startDate, endDate, interval, roi)\n","    samples = get_random_samples(roi)\n","    extract_points_value(s2SR_imgCol, samples,filename,folderName)\n","    print(filename)"],"metadata":{"id":"brLWXkE0cS_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## get ranom points in the sutdy region and get time series surface reflectance value of each random points in the reference year"],"metadata":{"id":"9shnLZu_duPH"}},{"cell_type":"code","source":["year = 2020\n","interval = 15\n","tilename = '30TYR'\n","s2_borderIndex = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/sentinel_2_index_shapefile\");\n","roi = s2_borderIndex.filter(ee.Filter.eq('Name', tilename))\n","get_attributes_ofSamples(year, interval, roi,tilename)"],"metadata":{"id":"62hZ11WacTsC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## get objects based on SNIC in the target year"],"metadata":{"id":"7KMX94OgnJkt"}},{"cell_type":"code","source":["def build_SNIC_objects (year, tilename):\n","    folderName = 'test_' + str(tilename)\n","    filename = f'SNIC_{tilename}_{year}'\n","    interval = 15\n","\n","    s2_borderIndex = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/sentinel_2_index_shapefile\");\n","    roi = s2_borderIndex.filter(ee.Filter.eq('Name', tilename))\n","    roi_geometry = roi.first().geometry()\n","\n","    startDate = ee.Date.fromYMD(year, 4, 1)\n","    endDate = ee.Date.fromYMD(year, 5, 1)\n","\n","    s2SR_imgCol = get_s2sr_images(startDate, endDate, interval, roi)\n","    s2SR_median = s2SR_imgCol.median()\n","    segmentation_features = s2SR_median.select(['R', 'NIR', 'SWIR1'])\n","\n","    seeds = ee.Algorithms.Image.Segmentation.seedGrid(36)\n","    snic = ee.Algorithms.Image.Segmentation.SNIC(\n","        image=segmentation_features,\n","        size=36,\n","        compactness=5,\n","        connectivity=8,\n","        neighborhoodSize=256,\n","        seeds=seeds\n","    )\n","\n","    clusters = snic.select('clusters') \\\n","                   .reproject(segmentation_features.select(0).projection(), None, 10) \\\n","                   .toInt() \\\n","                   .rename('clusters')\n","\n","    task_Drive = Export.image.toDrive(\n","        image=clusters,\n","        description=filename,\n","        folder=folderName,\n","        fileNamePrefix=filename,\n","        region=roi_geometry,\n","        scale=10,\n","        maxPixels=1e10\n","    )\n","    task_Drive.start()\n","\n","    assetId = f'projects/crops-mapping-gaoyuan/assets/SNIC_{tilename}_{year}'\n","    task_Asset = Export.image.toAsset(\n","        image=clusters,      # 你的整型标签影像\n","        description=filename,\n","        assetId=assetId,\n","        region=roi_geometry,             # 注意：必须是 Geometry，而不是 FeatureCollection\n","        scale=10,\n","        maxPixels=1e10   # 建议：标签类数据用 mode，避免重采样成浮点\n","    )\n","    task_Asset.start()\n","\n","    print('SNIC object get done!')\n","    return clusters\n"],"metadata":{"id":"VneFOg_XOe9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 生成对象质心点：数值显式转型\n","def clusters_to_centroids(clusters, roi):\n","    vecs = clusters.reduceToVectors(\n","        geometry=roi, scale=10, labelProperty='cluster',\n","        geometryType='centroid', eightConnected=True, maxPixels=1e13\n","    )\n","    # 给每个 feature 添加索引\n","    def add_index(feature, index):\n","        ll = feature.geometry().coordinates()\n","        return (feature\n","                .set('FID', index)  # 用 sequence 的序号代替 cluster\n","                .set('longitude', ee.Number(ll.get(0)))\n","                .set('latitude', ee.Number(ll.get(1))))\n","\n","    indexed = ee.FeatureCollection(\n","        ee.List(vecs.toList(vecs.size()))\n","          .zip(ee.List.sequence(0, vecs.size().subtract(1)))\n","          .map(lambda pair: add_index(ee.Feature(ee.List(pair).get(0)),\n","                                      ee.List(pair).get(1)))\n","    )\n","\n","    centroids = indexed.select(['FID', 'longitude', 'latitude', 'cluster'])\n","\n","    # 在函数内部增加导出任务\n","    task = ee.batch.Export.table.toAsset(\n","        collection=centroids,\n","        description='SNIC_30TYR_2021_point',\n","        assetId='projects/crops-mapping-gaoyuan/assets/SNIC_30TYR_2021_point'\n","    )\n","    task.start()  # 启动任务\n","\n","    return centroids\n","\n","def to_object_median_image(img, clusters):\n","    # 按 SNIC 对象聚合到对象尺度（中值）\n","    vals = img.addBands(clusters)\n","    obj_med = vals.reduceConnectedComponents(\n","        reducer   = ee.Reducer.median(),\n","        labelBand = 'cluster'\n","    )\n","    return obj_med.copyProperties(img, ['system:time_start'])\n","\n","# 采样：固定选择标量波段 + 明确日期为字符串\n","def sample_timeseries_by_points(imgCol, pts, fileName, folderName):\n","    # 按时序逐景取样到点；不做 toBands\n","    def _loop(img, ini):\n","        ini_fc = ee.FeatureCollection(ini)\n","        date = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd')\n","        ft = img.sampleRegions(\n","            collection = pts,\n","            properties = ['FID','longitude','latitude'],  # 若有标签，加 'Label'\n","            scale      = 10,\n","            geometries = False\n","        ).map(lambda f: f.set('date', date))\n","        return ini_fc.merge(ft)\n","\n","    out_fc = ee.FeatureCollection(imgCol.iterate(_loop, ee.FeatureCollection([])))\n","\n","    task = ee.batch.Export.table.toDrive(\n","        collection  = out_fc,\n","        description = fileName,\n","        folder      = folderName,\n","        fileFormat  = 'CSV'\n","    )\n","    task.start()\n","    return out_fc\n","\n","def get_attribute_fromObjects(year, interval, roi, tilename, SNIC_objects):\n","    folderName = f'test_{tilename}'\n","    filename = f'primary_{tilename}_{year}_objects'\n","\n","    startDate = ee.Date.fromYMD(year-1, 10, 1)\n","    endDate   = ee.Date.fromYMD(year,   10, 1)\n","\n","    s2SR_imgCol = get_s2sr_images(startDate, endDate, interval, roi)\n","\n","    clusters = SNIC_objects.select('clusters').toInt().rename('cluster')\n","\n","    # 与 clusters 投影一致\n","    s2 = s2SR_imgCol.map(lambda img: img.reproject(clusters.projection()))\n","    print('s2:',ee.Image(s2.first()).bandNames().getInfo())\n","\n","    # 每景对象中值\n","    objCol = s2.map(lambda img: to_object_median_image(img, clusters))\n","\n","    # 质心\n","    #objPts = clusters_to_centroids(clusters, roi)\n","    objPts = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/SNIC_30TYR_2021_point\")\n","\n","    # 采样 + 导出\n","    samples = sample_timeseries_by_points(\n","        imgCol=objCol, pts=objPts, fileName=filename, folderName=folderName\n","    )\n","    return {'objCol': objCol, 'objPts': objPts, 'samples': samples}\n"],"metadata":{"id":"w3Gr8TEzc5ye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["year = 2021\n","interval = 15\n","tilename = '30TYR'\n","s2_borderIndex = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/sentinel_2_index_shapefile\");\n","roi = s2_borderIndex.filter(ee.Filter.eq('Name', tilename))\n","\n","#SNIC_objects = build_SNIC_objects (year, tilename)\n","\n","SNIC_objects = ee.Image(\"projects/crops-mapping-gaoyuan/assets/SNIC_30TYR_2021\")\n","get_attribute_fromObjects(year, interval, roi, tilename, SNIC_objects)"],"metadata":{"id":"7mOK-vyqnR4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. get reference curve library (python)"],"metadata":{"id":"7b6F2ZhGaELw"}},{"cell_type":"markdown","source":["## get each reference points curve in the reference year"],"metadata":{"id":"tud-BrNNoo4C"}},{"cell_type":"code","source":["! pip install earthengine-api\n","import ee\n","ee.Authenticate()\n","ee.Initialize(project='crops-mapping-gaoyuan')\n","!pip install geemap\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install rasterio\n","!pip install cleanlab\n","import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime\n","import gc\n","import rasterio\n","import traceback\n","from scipy.optimize import curve_fit\n","from scipy.signal import savgol_filter\n","from scipy.optimize import curve_fit\n","from cleanlab.classification import CleanLearning\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","from scipy.optimize import curve_fit\n","from datetime import datetime\n","from scipy.signal import savgol_filter"],"metadata":{"id":"LyWlpw4fei1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get winter wheat based on the refer map in 2022\n","def get_crop_label(df, cl_file, chunk_size=5000):\n","    # 提取唯一的 FID、latitude 和 longitude\n","    unique_fid_lat_lon = df[['FID', 'latitude', 'longitude']].drop_duplicates()\n","\n","    # 构建点集合，并添加 FID 作为属性\n","    points = ee.FeatureCollection([\n","        ee.Feature(\n","            ee.Geometry.Point(row['longitude'], row['latitude']),\n","            {'FID': row['FID']}\n","        )\n","        for _, row in unique_fid_lat_lon.iterrows()\n","    ])\n","\n","    # 加载资产\n","    wheat_map = ee.Image(\"projects/crops-mapping-gaoyuan/assets/30TYR_RPG_2020_label\")\n","\n","    # 分块提取的函数\n","    def process_points_in_chunks(feature_collection, chunk_size):\n","        features = feature_collection.toList(feature_collection.size())\n","        num_chunks = (feature_collection.size().getInfo() + chunk_size - 1) // chunk_size\n","        results = []\n","        for i in range(num_chunks):\n","            # 当前分块\n","            chunk = ee.FeatureCollection(features.slice(i * chunk_size, (i + 1) * chunk_size))\n","\n","            # 对当前分块执行 reduceRegions\n","            sampled_chunk = wheat_map.reduceRegions(\n","                collection=chunk,\n","                reducer=ee.Reducer.first(),\n","                scale=10  # 替换为影像的分辨率\n","            )\n","\n","            # 转换当前分块为 Python 数据\n","            sampled_data = sampled_chunk.getInfo()['features']\n","            for feature in sampled_data:\n","                geom = feature['geometry']['coordinates']\n","                props = feature['properties']\n","                results.append({\n","                    'longitude': geom[0],\n","                    'latitude': geom[1],\n","                    'FID': props['FID'],\n","                    'Label': props.get('first', None)\n","                })\n","        return results\n","\n","    # 调用分块提取函数\n","    sampled_data = process_points_in_chunks(points, chunk_size)\n","\n","    # 转换为 Pandas DataFrame\n","    results = pd.DataFrame(sampled_data)\n","\n","    # 合并结果到原始数据\n","    merged = unique_fid_lat_lon.merge(results, on=['FID', 'latitude', 'longitude'], how='left')\n","\n","    # 保存结果\n","    merged.to_csv(cl_file, index=False)\n","\n","def sdiv(num, den):\n","        # 安全除法：分母接近 0 时置 NaN，避免 inf\n","        return num / den.mask(den.abs() < eps)\n","# add VIs\n","def add_VIs(df):\n","    # NDVI\n","    df['NDVI'] = 1000 * (df['NIR'] - df['R']) / (df['NIR'] + df['R'])\n","    # EVI (Enhanced Vegetation Index)\n","    df['EVI'] = 1000 * 2.5 * (df['NIR'] - df['R']) / (df['NIR'] + 6 * df['R'] - 7.5 * df['B'] + 10000)\n","    # LSWI (Land Surface Water Index)\n","    df['LSWI'] = 1000 * (df['NIR'] - df['SWIR1']) / (df['NIR'] + df['SWIR1'])\n","    # RVI (Ratio Vegetation Index)\n","    df['RVI'] = 1000 * (df['RE3'] / df['G'])\n","    # OSAVI (Optimized Soil-Adjusted Vegetation Index)\n","    df['OSAVI'] = 1000 * (df['NIR'] - df['R']) / (df['NIR'] + df['R'] + 1600)\n","    # GCVI (Green Chlorophyll Vegetation Index)\n","    df['GCVI'] = 1000 * (df['NIR'] / df['G'] - 1)\n","    # GVMI (Global Vegetation Moisture Index)\n","    df['GVMI'] = 1000 * ((df['NIR'] + 1000) - (df['SWIR1'] + 200)) / ((df['NIR'] + 1000) + (df['SWIR1'] + 200))\n","    # NDRE (Normalized Difference Red-Edge)\n","    df['NDRE'] = 1000 * (df['RE2'] - df['RE1']) / (df['RE2'] + df['RE1'])\n","    # REP (red edge position)\n","    df['REP'] = 705 + ((35*(0.5*(df['R'] + df['RE3'])- df['RE1'])) / (df['RE2'] - df['RE1']))\n","    return df\n","\n","def add_VIs_safe(df, scale=1000, eps=1e-6):\n","    # 必要列检查\n","    need = {'NIR','R','B','SWIR1','RE3','G','RE2','RE1'}\n","    miss = need - set(df.columns)\n","    if miss:\n","        raise KeyError(f\"Missing columns: {sorted(miss)}\")\n","\n","    # 转浮点，先清理明显异常反射率（按你数据实际可调整）\n","    X = df.copy()\n","    for c in need:\n","        X[c] = pd.to_numeric(X[c], errors='coerce').astype('float64')\n","    # 可选：把不合理值置 NaN（若反射率已×10000，范围可用 [-1000, 20000]）\n","    for c in need:\n","        X.loc[(X[c] < -1000) | (X[c] > 20000), c] = np.nan\n","\n","    def sdiv(num, den):\n","        # 安全除法：分母接近 0 时置 NaN，避免 inf\n","        return num / den.mask(den.abs() < eps)\n","\n","    # 指数计算（保持你的缩放与常量）\n","    X['NDVI'] = scale * sdiv(X['NIR'] - X['R'], X['NIR'] + X['R'])\n","    X['EVI']  = scale * 2.5 * sdiv(X['NIR'] - X['R'], X['NIR'] + 6*X['R'] - 7.5*X['B'] + 10000)\n","    X['LSWI'] = scale * sdiv(X['NIR'] - X['SWIR1'], X['NIR'] + X['SWIR1'])\n","    X['RVI']  = scale * sdiv(X['RE3'], X['G'])\n","    X['OSAVI']= scale * sdiv(X['NIR'] - X['R'], X['NIR'] + X['R'] + 1600)\n","    X['GCVI'] = scale * (sdiv(X['NIR'], X['G']) - 1.0)\n","    X['GVMI'] = scale * sdiv((X['NIR'] + 1000) - (X['SWIR1'] + 200),\n","                             (X['NIR'] + 1000) + (X['SWIR1'] + 200))\n","    X['NDRE'] = scale * sdiv(X['RE2'] - X['RE1'], X['RE2'] + X['RE1'])\n","\n","    # REP：分母过小置 NaN，保持 nm 不缩放\n","    den_rep = (X['RE2'] - X['RE1']).mask((X['RE2'] - X['RE1']).abs() < eps)\n","    X['REP'] = 705.0 + (35.0 * (0.5*(X['R'] + X['RE3']) - X['RE1']) / den_rep)\n","\n","    # 替换 inf/-inf→NaN\n","    vi_cols = ['NDVI','EVI','LSWI','RVI','OSAVI','GCVI','GVMI','NDRE','REP']\n","    X[vi_cols] = X[vi_cols].replace([np.inf, -np.inf], np.nan)\n","\n","    # 物理/经验裁剪（按×1000 缩放）\n","    clip_map = {\n","        'NDVI': (-scale, scale),\n","        'EVI':  (-scale, scale),\n","        'LSWI': (-scale, scale),\n","        'OSAVI':(-scale, scale),\n","        'GVMI': (-scale, scale),\n","        'NDRE': (-scale, scale),\n","        # RVI/GCVI 常为正，给宽松上限以防少量异常\n","        'RVI':  (0, 10*scale),\n","        'GCVI': (-scale, 10*scale),\n","        # REP 波段位置合理区间\n","        'REP':  (680, 750),\n","    }\n","    for k,(lo,hi) in clip_map.items():\n","        X[k] = X[k].clip(lower=lo, upper=hi)\n","\n","    # 可选：用中位数填补，或留 NaN 由下游处理\n","    # from sklearn.impute import SimpleImputer\n","    # imp = SimpleImputer(strategy='median')\n","    # X[vi_cols] = imp.fit_transform(X[vi_cols])\n","\n","    # 回写到原 df\n","    df[vi_cols] = X[vi_cols].astype({'REP':'float32', **{c:'float32' for c in vi_cols if c!='REP'}})\n","    return df\n","\n","# function of S-G filtering\n","def SG_filtering(df, bandList,SG_file):\n","    # 获取唯一的FID和日期\n","    FIDs = df['FID'].unique()\n","    #df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%dT%H:%M:%S')  #reference year\n","\n","    # 处理目标年份波段数据时需要使用下述date数据模式替换上述date数据模式\n","    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')  # target year\n","\n","    # 计算天数 (DOY)\n","    start_date = df['date'].min()\n","    base_date = datetime(start_date.year, 1, 1)\n","    df['doy'] = (df['date'] - base_date).dt.days + 1\n","    doys = df['doy'].unique()\n","\n","    # 临时存储处理后的数据\n","    temp_data_df = pd.DataFrame()\n","\n","    for bandname in bandList:\n","        # 初始化每个band的完整数据框\n","        temp_data_list = []\n","\n","        # 为每个DOY创建每个FID的列，并填充缺失值为NaN\n","        for doy in doys:\n","            # 创建一个包含所有FID的空列，默认值为NaN\n","            temp_doy_df = pd.DataFrame({'FID': FIDs})\n","\n","            # 从原始数据中获取对应DOY的记录，并填充到temp_doy_df\n","            time_column_name = f'{bandname}_{int(doy)}'\n","            band_data = df[df['doy'] == doy][['FID', bandname]].rename(columns={bandname: time_column_name})\n","            temp_doy_df = temp_doy_df.merge(band_data, on='FID', how='left')\n","            temp_data_list.append(temp_doy_df)\n","\n","        # 合并所有DOY数据\n","        combined_data = pd.concat(temp_data_list, axis=1, join='outer')\n","\n","        # 确保没有重复列名\n","        combined_data = combined_data.loc[:, ~combined_data.columns.duplicated()]\n","\n","        # 仅保留FID存在的数据\n","        combined_data = combined_data[combined_data['FID'].isin(FIDs)]\n","\n","        # 对缺失值进行线性插值\n","        vi_columns = [col for col in combined_data.columns if col.startswith(bandname)]\n","        combined_data[vi_columns] = combined_data[vi_columns].interpolate(method='linear', axis=1, limit_direction='both')\n","\n","        # 确保数据列数足够应用 Savitzky-Golay 滤波器\n","        window_length = 5  # 滑动窗口大小\n","        polyorder = 2      # 多项式阶数\n","\n","        if len(vi_columns) >= window_length:\n","            # 使用 Savitzky-Golay 滤波器进行平滑\n","            combined_data[vi_columns] = combined_data[vi_columns].apply(\n","                lambda x: savgol_filter(x.ffill().bfill(), window_length, polyorder)\n","                if x.notna().sum() >= window_length else x\n","             )\n","            # combined_data[vi_columns] = combined_data[vi_columns].apply(\n","            #     lambda x: fit_harmonic_curve(np.arange(len(x)), x.ffill().bfill())\n","            #     if x.notna().sum() >= 3 else x  # 确保有足够的数据点\n","            # )\n","\n","        # 确保平滑后数据没有NaN\n","        combined_data[vi_columns] = combined_data[vi_columns].ffill().bfill()\n","\n","        # 重设索引，确保 temp_data_df 和 combined_data 的行数一致\n","        combined_data = combined_data.reset_index(drop=True)\n","\n","        # 第一次赋值时直接赋值 temp_data_df\n","        if temp_data_df.empty:\n","            temp_data_df = combined_data[vi_columns]\n","        else:\n","            # 合并数据\n","            temp_data_df = pd.concat([temp_data_df, combined_data[vi_columns]], axis=1)\n","\n","    # 重置索引并添加FID列\n","    temp_data_df = temp_data_df.reset_index(drop=True)\n","    temp_data_df['FID'] = FIDs\n","\n","    temp_data_df.to_csv(SG_file, index=False)\n","\n","# main process to get the reference curve\n","def process_tileFile(filename,start_date,end_date,sr_bands,vi_bands,out_filenameSuffix):\n","    VI_SG_file = out_filenameSuffix + 'VI_SGfeatures.csv'\n","    SR_SG_file = out_filenameSuffix + 'SR_SGfeatures.csv'\n","    cl_file = out_filenameSuffix + 'cropLabel.csv'\n","\n","    primary_data = pd.read_csv(filename)\n","    data = primary_data.drop(['system:index','.geo'], axis=1)\n","\n","    # filter related time data for harmonic filtering\n","    #data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%dT%H:%M:%S') # reference year\n","    # 处理目标年份波段数据时需要使用下述date数据模式替换上述date数据模式\n","    data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')  # target year\n","\n","    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)].copy()\n","\n","    # convert date to doy for x vlues input for harmonic function\n","    base_date = datetime(start_date.year,1,1)\n","    filtered_data['doy'] = (filtered_data['date'] - base_date).dt.days + 1\n","\n","    # get reference wheat label of 2022\n","    #get_crop_label(filtered_data,cl_file)\n","    print('  get crop label done.')\n","    # get S-G filtering surface value\n","    SG_filtering(filtered_data,sr_bands,SR_SG_file)\n","    print('  get sr sg filtering done.')\n","    filtered_data_vis = add_VIs(filtered_data)\n","    SG_filtering(filtered_data_vis,vi_bands,VI_SG_file)\n","    print('  get vi sg filtering done.')"],"metadata":{"id":"Tq3xvRUJaOvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["year = 2020\n","tilename = '30TYR'\n","filename = f'/content/drive/MyDrive/test_{tilename}/primary_{tilename}_{year}.csv'\n","output_fileSuff = f'/content/drive/MyDrive/test_{tilename}/{tilename}_{year}_'\n","start_date = datetime(year-1, 9, 1)\n","end_date = datetime(year, 9, 1)\n","SR_bands = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n","VI_bands = ['NDVI','EVI','LSWI','OSAVI','RVI','GCVI','GVMI','NDRE','REP']\n","process_tileFile(filename,start_date,end_date,SR_bands,VI_bands,output_fileSuff)"],"metadata":{"id":"KH8_56kMjqS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## target sr pre-procession in target year"],"metadata":{"id":"rvFGyQ7d29c8"}},{"cell_type":"code","source":["year = 2021\n","tilename = '30TYR'\n","filename = f'/content/drive/MyDrive/test_{tilename}/primary_{tilename}_{year}_objects.csv'\n","output_fileSuff = f'/content/drive/MyDrive/test_{tilename}/{tilename}_{year}_'\n","start_date = datetime(year-1, 9, 1)\n","end_date = datetime(year, 9, 1)\n","SR_bands = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n","VI_bands = ['NDVI','EVI','LSWI','OSAVI','RVI','GCVI','GVMI','NDRE','REP']\n","process_tileFile(filename,start_date,end_date,SR_bands,VI_bands,output_fileSuff)"],"metadata":{"id":"ncQj66rr27eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## confidence learning to remove noise points"],"metadata":{"id":"5wVZhn1CoyeV"}},{"cell_type":"code","source":["import glob\n","import os\n","import rasterio\n","import numpy as np\n","import pandas as pd\n","from cleanlab.classification import CleanLearning\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","from scipy.optimize import curve_fit\n","from datetime import datetime\n","from scipy.signal import savgol_filter\n","import re\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"00lhwVvdcczt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_referenceLabel(tilename, year, debug=True, write_report=True):\n","    out_dir = f'/content/drive/MyDrive/test_{tilename}'\n","    out_filepath = f'{out_dir}/{tilename}_{year}_cleaned_labels.csv'\n","    report_path = f'{out_dir}/{tilename}_{year}_bad_features_report.csv'\n","\n","    label_file = f'{out_dir}/{tilename}_{year}_cropLabel.csv'\n","    VI_file = f'{out_dir}/{tilename}_{year}_VI_SGfeatures.csv'\n","    SR_file = f'{out_dir}/{tilename}_{year}_SR_SGfeatures.csv'\n","\n","    label_df = pd.read_csv(label_file, index_col='FID')\n","    VI_df = pd.read_csv(VI_file)\n","    SR_df = pd.read_csv(SR_file)\n","    feature_df = pd.merge(VI_df, SR_df, on='FID')\n","\n","    # 选列\n","    pat = re.compile(r'^(EVI|LSWI|OSAVI|RVI|R|NIR|SWIR1|SWIR2)_(\\d+)$')\n","    selected_columns = [c for c in feature_df.columns if pat.match(c)]\n","    print(f\"Selected cols: {len(selected_columns)}\")\n","\n","    if debug:\n","        print(f\"Total cols: {len(feature_df.columns)}, Selected cols: {len(selected_columns)}\")\n","        if len(selected_columns) == 0:\n","            print(\"Sample of columns:\", feature_df.columns[:50].tolist())\n","            raise ValueError(\"selected_columns is empty. Check prefixes / column names.\")\n","\n","    # 与标签对齐\n","    fid_column = label_df.index\n","    cdl_labels_binary = pd.to_numeric(label_df['Label'], errors='coerce').astype('Int64')\n","    feature_df = feature_df[feature_df['FID'].isin(fid_column)]\n","    valid_fids = cdl_labels_binary[cdl_labels_binary.notna()].index\n","\n","    X = feature_df.set_index('FID').loc[valid_fids, selected_columns].astype('float64').copy()\n","    y = cdl_labels_binary.loc[valid_fids].astype(int)\n","\n","    if debug:\n","        print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n","        uniq = np.unique(y)\n","        print(\"y unique:\", uniq)\n","\n","    # —— 诊断非有限/超界值 —— #\n","    max32 = np.finfo(np.float32).max\n","    vals = X.values\n","    mask_inf = np.isinf(vals)\n","    mask_nan = np.isnan(vals)\n","    mask_big = np.abs(vals) > max32\n","    mask_bad = mask_inf | mask_nan | mask_big\n","\n","    bad_rows = mask_bad.any(axis=1)\n","    bad_cols = mask_bad.any(axis=0)\n","\n","    n_bad_rows = int(bad_rows.sum())\n","    n_bad_cols = int(bad_cols.sum())\n","\n","    if debug:\n","        print(f\"Bad rows: {n_bad_rows}, Bad cols: {n_bad_cols}\")\n","        if n_bad_cols:\n","            bad_cols_counts = pd.Series(mask_bad.sum(axis=0), index=X.columns).sort_values(ascending=False)\n","            print(\"Top bad columns:\\n\", bad_cols_counts[bad_cols_counts > 0].head(20))\n","\n","        if n_bad_rows:\n","            bad_fids = X.index[bad_rows]\n","            print(\"First bad FIDs:\", bad_fids[:10].tolist())\n","            # 展示每个坏行的具体坏列（前 5 个）\n","            for fid in bad_fids[:5]:\n","                ri = X.index.get_loc(fid)\n","                cols = [c for c, b in zip(X.columns, mask_bad[ri]) if b]\n","                print(f\"FID {fid} bad cols: {cols[:15]}\")\n","\n","    # 写出完整报告并中断，防止进入 cl.fit 再报错\n","    if n_bad_rows or n_bad_cols:\n","        if write_report:\n","            records = []\n","            row_idx = np.where(bad_rows)[0]\n","            for r in row_idx:\n","                fid = X.index[r]\n","                cols = [c for c, b in zip(X.columns, mask_bad[r]) if b]\n","                records.append({\"FID\": fid, \"bad_cols\": \"|\".join(cols)})\n","            pd.DataFrame(records).to_csv(report_path, index=False)\n","            print(\"Bad-feature report ->\", report_path)\n","        raise ValueError(\"Features contain inf/NaN/too-large values. See printed diagnostics/report.\")\n","\n","    # —— 无问题后再训练 —— #\n","    rf_classifier = RandomForestClassifier()\n","    cl = CleanLearning(clf=rf_classifier)\n","\n","    noise_threshold = 50\n","    max_iterations = 20\n","\n","    #previous_labels = y.copy()\n","    le = LabelEncoder()\n","    y_enc = pd.Series(le.fit_transform(y.values), index=y.index)  # 0..K-1\n","    K = len(le.classes_)\n","    previous_labels = y_enc.copy()\n","\n","    for iteration in range(max_iterations):\n","        if debug: print(f\"Fitting iteration {iteration+1} ...\")\n","\n","        # 每轮新建 CleanLearning，避免带入上轮的 K\n","        class_counts = np.bincount(previous_labels.values, minlength=K)\n","        minc = int(class_counts.min())\n","        if minc < 2:\n","            if debug: print(f\"Stop at iter {it+1}: min class count < 2\")\n","            break\n","        cv = min(5, minc)\n","\n","        cl = CleanLearning(clf=RandomForestClassifier(), cv_n_folds=cv, verbose=debug)\n","        cl.fit(X, previous_labels.values)\n","        li = cl.find_label_issues(X=X, labels=previous_labels.values)\n","\n","        idx_issue = np.where(li['is_label_issue'].values)[0]\n","        preds = li['predicted_label'].astype(int).values\n","\n","        next_labels = previous_labels.copy()\n","        if len(idx_issue) > 0:\n","            next_labels.iloc[idx_issue] = preds[idx_issue]\n","\n","        previous_labels = next_labels\n","\n","        current_noise_count = int(li['is_label_issue'].sum())\n","        print(f\"Iteration {iteration + 1}: Noise points = {current_noise_count}\")\n","        if current_noise_count < noise_threshold:\n","            print(f\"Stopped at iteration {iteration + 1}: Noise < {noise_threshold}\")\n","            break\n","\n","    # 噪声标记在编码域比较\n","    is_noise_series = (previous_labels != y_enc).astype(int)\n","\n","    # 导出前把置信标签反编码回原始标签域\n","    conf_label_enc = previous_labels.reindex(label_df.index)                     # 对齐导出索引\n","    mask = conf_label_enc.notna()\n","    conf_label = pd.Series(index=label_df.index, dtype='float64')\n","    conf_label[mask] = le.inverse_transform(conf_label_enc[mask].astype(int).values)\n","\n","    # is_noise 与导出索引对齐\n","    is_noise = is_noise_series.reindex(label_df.index).fillna(1).astype(int).to_numpy()\n","\n","    result_df = pd.DataFrame({\n","        'FID': label_df.index,\n","        'latitude': label_df['latitude'],\n","        'longitude': label_df['longitude'],\n","        'Map_Label': label_df['Label'],                 # 原始域\n","        'Confidence_Label': conf_label.to_numpy(),      # 反编码后标签\n","        'Is_Noise': is_noise\n","    })\n","    result_df.to_csv(out_filepath, index=False)\n","    if debug: print(\"Saved cleaned labels ->\", out_filepath)"],"metadata":{"id":"se8YIOb7t0Qc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tilename = '30TYR'\n","year = 2020\n","clean_referenceLabel(tilename,year)"],"metadata":{"id":"5lX75dDoe2B9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## reclustering"],"metadata":{"id":"V8TD8w5Ro5hi"}},{"cell_type":"code","source":["# reclustering the wheat samples and non wheat samples using EVI value after SG filtering\n","def reclustering(tilename, year, debug=True):\n","    out_dir = f'/content/drive/MyDrive/test_{tilename}'\n","    out_filepath = f'{out_dir}/{tilename}_{year}_clustered_labels.csv'\n","\n","    result_file = f'{out_dir}/{tilename}_{year}_cleaned_labels.csv'\n","    VI_file     = f'{out_dir}/{tilename}_{year}_VI_SGfeatures.csv'\n","\n","    # 读数据\n","    result_df  = pd.read_csv(result_file)\n","    feature_df = pd.read_csv(VI_file)\n","\n","    # 确保用 FID 对齐\n","    if 'FID' not in result_df.columns or 'FID' not in feature_df.columns:\n","        raise KeyError(\"Both files must contain 'FID' column.\")\n","    feature_df = feature_df.set_index('FID')\n","    result_df  = result_df.set_index('FID', drop=False)\n","\n","    # 选 EVI_* 特征\n","    evi_cols = [c for c in feature_df.columns if re.match(r'^EVI_\\d+$', c)]\n","    if debug:\n","        print(\"EVI cols:\", len(evi_cols))\n","    if not evi_cols:\n","        raise ValueError(\"No EVI_* columns found.\")\n","\n","    features_VI = feature_df[evi_cols].astype('float64')\n","    # 缺失填补（稳健）\n","    if features_VI.isna().any().any():\n","        imp = SimpleImputer(strategy='median')\n","        features_VI = pd.DataFrame(imp.fit_transform(features_VI),\n","                                   index=features_VI.index, columns=features_VI.columns)\n","\n","    # 过滤噪声：兼容 0/1 或 True/False\n","    if result_df['Is_Noise'].dropna().isin([0,1]).all():\n","        keep_mask = result_df['Is_Noise'] == 0\n","    else:\n","        keep_mask = result_df['Is_Noise'] == False\n","\n","    filtered_data = result_df[keep_mask].copy()\n","    filtered_features = features_VI.loc[filtered_data.index.intersection(features_VI.index)]\n","\n","    if debug:\n","        print(\"After noise filter:\", filtered_data.shape[0], \"samples\")\n","\n","    # 二值化标签：label==1 -> 1，否则 0\n","    if 'Confidence_Label_Bin' in filtered_data.columns:\n","        y_bin = filtered_data['Confidence_Label_Bin'].astype(int)\n","    elif 'Confidence_Label' in filtered_data.columns:\n","        y_bin = (pd.to_numeric(filtered_data['Confidence_Label'], errors='coerce').fillna(-1).astype(int) == 1).astype(int)\n","    elif 'Map_Label' in filtered_data.columns:\n","        y_bin = (pd.to_numeric(filtered_data['Map_Label'], errors='coerce').fillna(-1).astype(int) == 1).astype(int)\n","    else:\n","        raise KeyError(\"Need one of ['Confidence_Label_Bin','Confidence_Label','Map_Label'] in result file.\")\n","\n","    if debug:\n","        print(\"Binary label counts:\", y_bin.value_counts().to_dict())\n","\n","    # 拆分\n","    idx0 = filtered_data.index[y_bin == 0]\n","    idx1 = filtered_data.index[y_bin == 1]\n","    X0 = filtered_features.loc[idx0]\n","    X1 = filtered_features.loc[idx1]\n","\n","    def safe_kmeans(X, k, name):\n","        n = len(X)\n","        if n == 0:\n","            if debug: print(f\"{name}: 0 samples. Skip clustering.\")\n","            return np.array([], dtype=int)\n","        if k > n:\n","            if debug: print(f\"{name}: reduce n_clusters {k}->{n}\")\n","            k = n\n","        return KMeans(n_clusters=k, random_state=42).fit_predict(X)\n","\n","    # 聚类\n","    clusters_label_0 = safe_kmeans(X0, 5, \"label==0\")\n","    clusters_label_1 = safe_kmeans(X1, 2, \"label==1\")\n","\n","    # 生成聚类编码\n","    lab0 = pd.Series((clusters_label_0 + 1) * 10, index=idx0) if len(clusters_label_0) else pd.Series(dtype='float64')\n","    lab1 = pd.Series((clusters_label_1 + 1) * 10 + 1, index=idx1) if len(clusters_label_1) else pd.Series(dtype='float64')\n","\n","    # 写回\n","    result_df['Cluster'] = np.nan\n","    result_df.loc[lab0.index, 'Cluster'] = lab0\n","    result_df.loc[lab1.index, 'Cluster'] = lab1\n","\n","    # 导出\n","    final_cols = [c for c in ['FID','latitude','longitude','Map_Label','Confidence_Label','Confidence_Label_Bin','Cluster'] if c in result_df.columns]\n","    final_result = result_df[final_cols].reset_index(drop=True)\n","    final_result.to_csv(out_filepath, index=False)\n","    if debug:\n","        print(\"Saved ->\", out_filepath, \"| rows:\", len(final_result))\n"],"metadata":{"id":"2Y1E91_t2het"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tilename = '30TYR'\n","year = 2020\n","reclustering(tilename,year)"],"metadata":{"id":"v7vP3xS6_EDu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## generate reference curve"],"metadata":{"id":"jr3AvCBNd9iG"}},{"cell_type":"code","source":["#生成谐波参数名\n","def genetate_feature_names(bandname):\n","    feature_names = [f'constant_{bandname}_{bandname}', f'cos_1_{bandname}_{bandname}', f'cos_2_{bandname}_{bandname}', f'cos_3_{bandname}_{bandname}', f'sin_1_{bandname}_{bandname}', f'sin_2_{bandname}_{bandname}', f'sin_3_{bandname}_{bandname}']\n","    return feature_names\n","\n","# 定义谐波函数\n","def harmonic_function(x, feature_row,bandname):\n","    feature_name = genetate_feature_names(bandname)\n","    \"\"\" 构建谐波函数，基于给定特征 \"\"\"\n","    constant = feature_row[feature_name[0]]\n","    harmonic_sum = (\n","        constant +\n","        feature_row[feature_name[1]] * np.cos(x) +\n","        feature_row[feature_name[4]] * np.sin(x) +\n","        feature_row[feature_name[2]] * np.cos(2 * x) +\n","        feature_row[feature_name[5]] * np.sin(2 * x) +\n","        feature_row[feature_name[3]] * np.cos(3 * x) +\n","        feature_row[feature_name[6]] * np.sin(3 * x)\n","    )\n","    return harmonic_sum\n","\n","# 定义要拟合的谐波模型\n","def harmonic_model(x, constant, cos_1, cos_2, cos_3, sin_1, sin_2, sin_3):\n","    \"\"\" 拟合的谐波模型，基于谐波函数的形式 \"\"\"\n","    return (\n","        constant +\n","        cos_1 * np.cos(x) + sin_1 * np.sin(x) +\n","        cos_2 * np.cos(2 * x) + sin_2 * np.sin(2 * x) +\n","        cos_3 * np.cos(3 * x) + sin_3 * np.sin(3 * x)\n","    )\n","\n","# 获取谐波参数 based on VI value\n","def get_harmonic_parameter_usingValue(features_df,start_date,end_date,labels_df,bandname,labelColumn):\n","\n","    merged_df = features_df.merge(labels_df[['FID', labelColumn]], on='FID', how='inner')\n","\n","    # 定义时间序列的 x 轴范围 (比如从 0 到 2π)\n","    base_date = datetime(start_date.year,1,1)\n","    start_doy = (start_date- base_date).days + 1\n","    end_doy = (end_date- base_date).days + 1\n","    x_values = np.arange(start_doy-1, end_doy,15)\n","\n","    clusters = merged_df[labelColumn].dropna().unique()\n","    features_name = [col for col in features_df.columns if col.startswith(f'{bandname}_')]\n","\n","    # 提取 features_name 中的 DOY\n","    feature_doys = [int(col.split('_')[1]) for col in features_name]  # 提取 DOY 值\n","    feature_doys_set = set(feature_doys)\n","\n","    # 计算交集 DOY\n","    x_values_set = set(x_values)\n","    common_doys = feature_doys_set.intersection(x_values_set)\n","\n","    # 根据交集筛选 features_name 和 x_values\n","    filtered_features_name = [col for col in features_name if int(col.split('_')[1]) in common_doys]\n","    filtered_x_values = [doy for doy in x_values if doy in common_doys]\n","\n","    # 转换为 NumPy 数组（可选）\n","    filtered_x_values = np.array(filtered_x_values)\n","    t_values = filtered_x_values * np.pi / 365\n","\n","    all_popt = []\n","\n","    for cluster in clusters:\n","        # 筛选属于当前聚类的样本\n","        cluster_data = merged_df[merged_df[labelColumn] == cluster]\n","        features = cluster_data[filtered_features_name]\n","        features_mean = features.median()\n","        popt, pcov = curve_fit(harmonic_model, t_values, features_mean)\n","        all_popt.append((cluster, popt))\n","\n","    return all_popt\n","\n","# 导出谐波参数到csv文件\n","def export_cluster_parameters_to_csv(all_bands_popt, output_file):\n","    # Step 1: 获取所有波段名称\n","    bands = [band for band, _ in all_bands_popt]\n","\n","    # Step 2: 初始化存储结果的列表（存储列名）\n","    columns = ['Cluster']  # 第一列是 Cluster 名\n","    for band in bands:\n","        columns.extend([f'{band}_constant', f'{band}_cos1', f'{band}_cos2', f'{band}_cos3', f'{band}_sin1', f'{band}_sin2', f'{band}_sin3'])\n","\n","    # 存储每个 cluster 的行数据\n","    result_rows = []\n","\n","    # Step 3: 遍历所有波段，按 cluster 聚合数据\n","    # 假设 all_bands_popt 是 [(band_name, [(cluster1, popt1), (cluster2, popt2), ...]), ...] 形式的列表\n","    cluster_keys = list(set([cluster for _, clusters_popt in all_bands_popt for cluster, _ in clusters_popt]))  # 获取所有 cluster 的 key\n","\n","    for cluster in cluster_keys:\n","        row = [cluster]  # 初始化每行的第一个元素是 cluster 名\n","\n","        # 遍历每个波段的参数\n","        for band, band_popt in all_bands_popt:\n","            # 查找该 cluster 对应的 popt\n","            popt = next((popt for c, popt in band_popt if c == cluster), [None] * 7)  # 如果找不到 cluster，填充 None\n","\n","            # 确保 popt 的参数顺序为 [constant, cos1, cos2, cos3, sin1, sin2, sin3]\n","            row.extend(popt)\n","\n","        # 将该行数据添加到结果中\n","        result_rows.append(row)\n","\n","    # Step 4: 将数据转换为 pandas DataFrame 并导出为 CSV\n","    df = pd.DataFrame(result_rows, columns=columns)\n","    df.to_csv(output_file, index=False)\n","\n","# get reference curve of EVI and SR\n","def get_reference_curve_parameters(tilename,year):\n","\n","    start_date = datetime(year, 10, 1)\n","    end_date = datetime(year + 1, 7, 1)\n","    out_dir = f'/content/drive/MyDrive/test_{tilename}'\n","\n","    label_file = f'{out_dir}/{tilename}_{year}_cleaned_labels.csv'\n","    label_df = pd.read_csv(label_file)\n","\n","    VI_harmonicValue_file = f'{out_dir}/{tilename}_{year}_VI_SGfeatures.csv'\n","    VI_harmonicValue_df = pd.read_csv(VI_harmonicValue_file)\n","\n","    SR_harmonicValue_file = f'{out_dir}/{tilename}_{year}_SR_SGfeatures.csv'\n","    SR_harmonicValue_df = pd.read_csv(SR_harmonicValue_file)\n","\n","    SR_band = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n","    VI_band = ['EVI','LSWI','OSAVI','RVI']\n","\n","    output_SR_file = f'{out_dir}/{tilename}_{year}_SR_Reference_parameters.csv'\n","    output_VI_file = f'{out_dir}/{tilename}_{year}_VI_Reference_parameters.csv'\n","\n","    all_SRbands_popt = []\n","    all_VIbands_popt = []\n","\n","    labelColumn = 'Confidence_Label'\n","\n","    for band in SR_band:\n","        SR_parameter = get_harmonic_parameter_usingValue(SR_harmonicValue_df,start_date,end_date,label_df,band,labelColumn)\n","        all_SRbands_popt.append((band, SR_parameter))\n","    export_cluster_parameters_to_csv(all_SRbands_popt, output_SR_file)\n","    print('SR reference curve export done.')\n","\n","    for band in VI_band:\n","        VI_parameter = get_harmonic_parameter_usingValue(VI_harmonicValue_df,start_date,end_date,label_df,band,labelColumn)\n","        all_VIbands_popt.append((band, VI_parameter))\n","    export_cluster_parameters_to_csv(all_VIbands_popt, output_VI_file)\n","    print('VI reference curve export done.')\n"],"metadata":{"id":"rOZsGDkyeAta"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tilename = '30TYR'\n","year = 2020\n","get_reference_curve_parameters(tilename,year)"],"metadata":{"id":"3N6fbneEkb6L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## plot the reference curve"],"metadata":{"id":"hmC_Mb5KLVp5"}},{"cell_type":"code","source":["from datetime import datetime\n","def generate_parameter_names(bandname):\n","    feature_names = [f'{bandname}_constant', f'{bandname}_cos1', f'{bandname}_cos2', f'{bandname}_cos3', f'{bandname}_sin1', f'{bandname}_sin2', f'{bandname}_sin3']\n","    return feature_names\n","# 定义谐波函数\n","def harmonic_function(x, feature_row,bandname):\n","    feature_name = generate_parameter_names(bandname)\n","    \"\"\" 构建谐波函数，基于给定特征 \"\"\"\n","    constant = feature_row[feature_name[0]]\n","    harmonic_sum = (\n","        constant +\n","        feature_row[feature_name[1]] * np.cos(x) +\n","        feature_row[feature_name[4]] * np.sin(x) +\n","        feature_row[feature_name[2]] * np.cos(2 * x) +\n","        feature_row[feature_name[5]] * np.sin(2 * x) +\n","        feature_row[feature_name[3]] * np.cos(3 * x) +\n","        feature_row[feature_name[6]] * np.sin(3 * x)\n","    )\n","    return harmonic_sum\n","def plot_reference_curve(tilename):\n","    label_filepath = '/content/drive/MyDrive/test_30TYR/30TYR_2020_VI_Reference_parameters.csv'\n","    label_df = pd.read_csv(label_filepath)\n","    bands = ['EVI','LSWI','OSAVI','RVI']\n","    #bands = ['VV']\n","    feature_EVI_name = generate_parameter_names(bands[0])\n","    #clusters = [11,21,10,20,30,40,50]\n","    clusters = [1,2,3,4,5,6]\n","    start_date = datetime(2020, 10, 1)\n","    end_date = datetime(2021, 7, 1)\n","    base_date = datetime(start_date.year,1,1)\n","    start_doy = (start_date- base_date).days + 1\n","    end_doy = (end_date- base_date).days + 1\n","    x_values = np.arange(start_doy, end_doy + 1,15)\n","    t_values = x_values * np.pi / 365\n","\n","    cluster_name_map = {\n","        1: 'Winter Wheat',\n","        2: 'winter rapeseed',\n","        3: 'other winter crops',\n","        4: 'spring crops',\n","        5: 'summer crops',\n","        6: 'Other crops'\n","    }\n","\n","    for cluster in clusters:\n","        # 筛选属于当前聚类的样本\n","        cluster_data = label_df[label_df['Cluster'] == cluster]\n","\n","        # 初始化当前聚类的谐波曲线\n","        mean_harmonic_values = np.zeros_like(t_values)\n","\n","        # 对每个样本计算谐波函数并取平均\n","        for _, row in cluster_data[feature_EVI_name].iterrows():\n","            harmonic_values = harmonic_function(t_values, row,'EVI')\n","            mean_harmonic_values += harmonic_values\n","\n","        # 平均每个聚类的谐波曲线\n","        mean_harmonic_values /= len(cluster_data)\n","        print(cluster,' sample number:',len(cluster_data))\n","\n","        # 绘制聚类的平均谐波曲线\n","        plt.plot(x_values, mean_harmonic_values, label=cluster_name_map.get(cluster, f'Cluster {cluster}'))\n","\n","    # 添加图例、标题和标签\n","    plt.title('Harmonic Curves for Each Cluster')\n","    plt.xlabel('DOY')\n","    plt.ylabel('Harmonic Value')\n","    plt.legend()\n","    plt.show()\n","\n","plot_reference_curve('30TYR')"],"metadata":{"id":"VctDhMOALY_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. get training objects (GEE & python)"],"metadata":{"id":"RUeQG-DbbLN1"}},{"cell_type":"markdown","source":["## DTSS method to calculate similarity distance"],"metadata":{"id":"gIsxukJLU5iI"}},{"cell_type":"code","source":["!pip install rasterio matplotlib\n","!pip install tslearn joblib"],"metadata":{"id":"xXMN7AeMbc8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import glob\n","from tslearn.metrics import dtw_path\n","from joblib import Parallel, delayed\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","from scipy.signal import savgol_filter\n","from datetime import datetime"],"metadata":{"id":"M-S8aZiAsFjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#**** functions related to get refer VI curve\n","# add VIs\n","def add_VIs(df):\n","    # EVI (Enhanced Vegetation Index)\n","    df['EVI'] = 1000 * 2.5 * (df['NIR'] - df['R']) / (df['NIR'] + 6 * df['R'] - 7.5 * df['B'] + 10000)\n","    return df\n","\n","# generate harmonic parameter name based on a given bandname\n","def generate_parameter_names(bandname):\n","    feature_names = [f'{bandname}_constant', f'{bandname}_cos1', f'{bandname}_cos2', f'{bandname}_cos3', f'{bandname}_sin1', f'{bandname}_sin2', f'{bandname}_sin3']\n","    return feature_names\n","\n","# define the harmonic function\n","def harmonic_function(x, feature_row,bandname):\n","    parameter_names = generate_parameter_names(bandname)\n","    \"\"\" 构建谐波函数，基于给定特征 \"\"\"\n","    constant = feature_row[parameter_names[0]]\n","    VI_harmonic_values = (\n","        constant +\n","        feature_row[parameter_names[1]] * np.cos(x) +\n","        feature_row[parameter_names[4]] * np.sin(x) +\n","        feature_row[parameter_names[2]] * np.cos(2 * x) +\n","        feature_row[parameter_names[5]] * np.sin(2 * x) +\n","        feature_row[parameter_names[3]] * np.cos(3 * x) +\n","        feature_row[parameter_names[6]] * np.sin(3 * x)\n","    )\n","    return VI_harmonic_values\n","\n","#get refer vi curve of whole growing period based on the harmonic parameters\n","def get_refer_VIcurve(VI_file,t_values,bandname):\n","    VI_df = pd.read_csv(VI_file)\n","    x_values = t_values * np.pi / 365\n","\n","    refer_VI_curve_list = VI_df.apply(lambda row: harmonic_function(x_values, row, bandname), axis=1)\n","\n","    refer_VI_curve_df = pd.DataFrame(refer_VI_curve_list.tolist(), columns=[f'Time_{int(t)}' for t in t_values])\n","    refer_VI_curve_df['Cluster'] = VI_df['Cluster']\n","    return refer_VI_curve_df\n","\n","# get the spectral reflectance curve based on the harmonic parameters and given time\n","def get_refer_srCurve(refer_SRpara,SR_bandnames,start_date,time_Ts,interval):\n","    refer_SRpara = refer_SRpara.iloc[0]\n","    refer_SR_curve_list = []\n","    time_Ts = np.array(time_Ts)\n","\n","    start_doy = start_date.timetuple().tm_yday\n","    x = start_doy + (time_Ts-1) * interval * np.pi / 365\n","\n","    for sr_bandname in SR_bandnames:\n","        sr_value = harmonic_function(x, refer_SRpara,sr_bandname)\n","        refer_SR_curve_list.append(sr_value)\n","    refer_SR_curve_array = np.array(refer_SR_curve_list).T\n","    return refer_SR_curve_array\n","\n","# get the refer curve time of given time T based on the path result of dtw\n","def get_referTime(optimal_paths,time_T):\n","    refer_Ts = []\n","    for path in optimal_paths:\n","        refer_T = None\n","        for refer_index,sample_index in path:\n","            if sample_index == time_T:\n","                refer_T = refer_index\n","                break\n","        refer_Ts.append(refer_T)\n","    return refer_Ts\n","\n","# get the spectral reflectance curve of target samples based on the given filename\n","def get_target_srCurve(file,timeT,sr_bandnames,FID_df):\n","    primary_df = pd.read_csv(file)\n","    primary_df = primary_df.drop(['system:index','.geo'], axis=1)\n","    primary_df['date'] = pd.to_datetime(primary_df['date'], format='%Y-%m-%dT%H:%M:%S')\n","    target_sr_df = primary_df[(primary_df['date'] == timeT)]\n","    target_sr_df = target_sr_df[target_sr_df['FID'].isin(FID_df['FID'])]\n","    target_SR_curve_list = target_sr_df[sr_bandnames].values.tolist()\n","    return target_SR_curve_list\n","\n","# get the spectral reflectance curve of target samples based on the given filename after SG filtering\n","def get_target_srCurve_SG(file,start_date, timeT,sr_bandnames,FID_df):\n","    primary_df = pd.read_csv(file)\n","    base_date = datetime(start_date.year, 1, 1)\n","    doy = (timeT - base_date).days + 1\n","    sr_columns = [f'{band}_{doy}' for band in sr_bandnames]\n","\n","    existing_columns = [col for col in sr_columns if col in primary_df.columns]\n","    if not existing_columns:\n","        return []\n","    target_sr_df = primary_df[primary_df['FID'].isin(FID_df['FID'])]\n","    target_SR_curve_list = target_sr_df[sr_columns].values.tolist()\n","    return target_SR_curve_list\n","\n","#*** get target VI value of given time period based on the primary surface reflectance value\n","def get_target_VIs(target_features_file, VI_bandname, start_date, timeT):\n","    primary_data = pd.read_csv(target_features_file)\n","    primary_data = primary_data.drop(['system:index', '.geo'], axis=1)\n","    primary_data['date'] = pd.to_datetime(primary_data['date'], format='%Y-%m-%dT%H:%M:%S')\n","    timeT_df = primary_data[(primary_data['date'] == timeT)]\n","    time_T_FIDs = timeT_df['FID'].unique()\n","\n","    data = add_VIs(primary_data)\n","\n","    # Convert date to doy\n","    base_date = datetime(start_date.year, 1, 1)\n","    data['doy'] = (data['date'] - base_date).dt.days + 1\n","    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= timeT)].copy()\n","\n","    doys = filtered_data['doy'].unique()\n","    temp_data_list = []\n","\n","    # Prepare a list to store data for each doy\n","    for doy in doys:\n","        df = filtered_data[filtered_data['doy'] == doy]\n","        time_column_name = f'{VI_bandname}_{int(doy)}'\n","        df = df[['FID','latitude','longitude', VI_bandname]].rename(columns={VI_bandname: time_column_name})\n","        temp_data_list.append(df)\n","\n","    combined_data = pd.concat(temp_data_list, axis=1, join='outer')\n","    combined_data = combined_data.loc[:, ~combined_data.columns.duplicated()]\n","    combined_data = combined_data[combined_data['FID'].isin(time_T_FIDs)]\n","\n","    # Interpolate missing values in the VI columns\n","    vi_columns = [col for col in combined_data.columns if col.startswith(VI_bandname)]\n","    combined_data[vi_columns] = combined_data[vi_columns].interpolate(method='linear', axis=1, limit_direction='both')\n","    combined_data[vi_columns] = combined_data[vi_columns].fillna(method='ffill').fillna(method='bfill')\n","\n","    return combined_data\n","\n","#*** get target VI value of given time period based on the SG filtering value\n","def get_target_VIs_SG(target_features_file, FID_file,VI_bandname, start_date, timeT):\n","    FID_df = pd.read_csv(FID_file)\n","    FID_df = FID_df.drop_duplicates(subset='FID')\n","    FID_location = FID_df.set_index('FID')[['latitude', 'longitude']]\n","\n","    primary_data = pd.read_csv(target_features_file)\n","\n","    all_VI_columnsName = [col for col in primary_data.columns if col.startswith(f'{VI_bandname}_')]\n","    # get the given time period VI value\n","    base_date = datetime(start_date.year, 1, 1)\n","    start_doy = (start_date - base_date).days + 1\n","    timeT_doy = (timeT - base_date).days + 1\n","    doys = np.arange(start_doy, timeT_doy + 1,15)\n","\n","    filtered_columns = [col for col in all_VI_columnsName if int(col.split('_')[1]) in doys]\n","    existing_columns = [col for col in filtered_columns if col in primary_data.columns]\n","    if existing_columns:\n","        filtered_data = primary_data[['FID'] + existing_columns]\n","        filtered_data = filtered_data.copy()\n","        filtered_data['latitude'] = filtered_data['FID'].map(FID_location['latitude'])\n","        filtered_data['longitude'] = filtered_data['FID'].map(FID_location['longitude'])\n","    else:\n","        # 如果没有匹配列，返回一个空 DataFrame\n","        filtered_data = pd.DataFrame(columns=['FID', 'latitude', 'longitude'])\n","    return filtered_data\n","\n","#*** functions related of DTLS similarity calculation\n","# function to compute DTW distance and path for one tme series\n","def compute_dtw(reference,sample,search_radius):\n","    distance, path = dtw_path(reference, sample,global_constraint='sakoe_chiba',sakoe_chiba_radius=search_radius)\n","    return distance, path\n","\n","# function to compute SAD of the target surface reflectance at given time T and refer surface reflectance at related time refer_T\n","def compute_sad(refer_SRs, target_SRs):\n","    refer_SRs = np.array(refer_SRs)\n","    target_SRs = np.array(target_SRs)\n","    if refer_SRs.shape != target_SRs.shape:\n","        print('refer_SRs',refer_SRs.shape)\n","        print('target_SRs',target_SRs.shape)\n","        raise ValueError(\"refer_SRs 和 target_SRs 的形状必须匹配！\")\n","    dot_product = np.einsum('ij,ij->i', refer_SRs, target_SRs)\n","    refer_magnitudes = np.linalg.norm(refer_SRs, axis=1)\n","    target_magnitudes = np.linalg.norm(target_SRs, axis=1)\n","    # Avoid division by zero by setting SAD to 1.0 for zero-magnitude cases\n","    zero_mask = (refer_magnitudes == 0) | (target_magnitudes == 0)\n","    # Calculate cosine similarity\n","    cos_theta = np.divide(dot_product, refer_magnitudes * target_magnitudes, where=~zero_mask)\n","    # Clamp values to the range [-1, 1] to avoid any issues with arccos\n","    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n","    # Calculate the spectral angle distance in radians and normalize by pi, then scale by 10000\n","    spectral_angle_distance = 2 * np.arccos(cos_theta) / np.pi\n","    # Set SAD to 1.0 (or any indicator value you choose) where magnitudes are zero\n","    spectral_angle_distance[zero_mask] = 1.0\n","    return spectral_angle_distance*10000\n","\n","# DTLS method for sample similatity distance calculation for a given cluster and a given time T\n","def DTLS_singleCluster(refer_curve,target_curve_df,target_lastT_srCurve,start_date,timeT,timeT_index,refer_SRpara,SR_bandnames,search_radius,interval):\n","\n","    # convert the refer curve dataframe to array for dtw method input\n","    time_columns = [col for col in refer_curve.columns if col.startswith('Time_')]\n","\n","    refer_curve_array = refer_curve.iloc[0][time_columns].values\n","\n","    # convert the target curve dataframe to array for dtw method input\n","    vi_columns = [col for col in target_curve_df.columns if col.startswith('EVI_')]\n","    samples_curve_series = target_curve_df.loc[:, vi_columns].values\n","    extended_samples_curve_series = np.pad(\n","        samples_curve_series,\n","        pad_width=((0, 0), (search_radius, search_radius)),  # No padding on rows, padding on columns\n","        mode='edge'\n","    )\n","\n","    # Parallelize the DTW computation across multiple cores\n","    num_cores = -1  # Use all available cores. You can specify a number like num_cores=4 to limit.\n","    results = Parallel(n_jobs=num_cores)(delayed(compute_dtw)(refer_curve_array, sample,search_radius) for sample in extended_samples_curve_series)\n","    # Extract distances and paths from the results\n","    optimal_path,dtw_distance = zip(*results)\n","\n","    # get the refer surface reflectance curves of given last time using the harmonic parameters\n","    relavent_target_lastT = timeT_index + search_radius - 1\n","    refer_lastT = get_referTime(optimal_path,relavent_target_lastT)\n","    refer_lastT_srCurve = get_refer_srCurve(refer_SRpara,SR_bandnames,start_date,refer_lastT,interval)\n","\n","    # get the related spectral angle distance of related time period\n","    spectral_angle_distance = compute_sad(refer_lastT_srCurve,target_lastT_srCurve)\n","\n","    DTLS_distance = dtw_distance + spectral_angle_distance\n","\n","    return dtw_distance,spectral_angle_distance #DTLS_distance\n","\n","#*** main producer of DTLS for sample similarity distance calculation for each cluster at given time time_T\n","def DTLS_distance_calculate(target_features_fileDir,refer_SR_file,refer_VI_file,VI_bandname,SR_bandnames,out_fileDir,start_date,timeT,timeT_index,search_radius,tilname,interval,target_year):\n","    # get refer sr harmonic parameters\n","    refer_SRpara_df = pd.read_csv(refer_SR_file)\n","    # get refer vi curve\n","    referT_indexs = np.linspace(1-search_radius, timeT_index+search_radius, timeT_index+2*search_radius)\n","    start_doy = start_date.timetuple().tm_yday\n","    referT_values = start_doy + (referT_indexs-1)*interval\n","    refer_VI_curve_df = get_refer_VIcurve(refer_VI_file,referT_values,VI_bandname)\n","\n","    # get target vi value of each sample\n","    target_VI_file = os.path.join(target_features_fileDir, f'{tilname}_{target_year}_VI_SGfeatures.csv')\n","    FID_file = '/content/drive/MyDrive/test_30TYR/primary_30TYR_2021_objects.csv'\n","    target_VI_df = get_target_VIs_SG(target_VI_file,FID_file,VI_bandname,start_date,timeT)\n","\n","    if target_VI_df.empty:\n","        print(timeT,\" is empty.\")\n","        return\n","\n","    # get the target surface reflectance curve of given last time using the related sr file\n","    target_sr_file = os.path.join(target_features_fileDir, f'{tilname}_{target_year}_SR_SGfeatures.csv')\n","    target_lastT_srCurve = get_target_srCurve_SG(target_sr_file,start_date,timeT,SR_bandnames,target_VI_df)\n","\n","    sample_cluster_resultDF = pd.DataFrame()\n","    sample_cluster_resultDF['FID'] = target_VI_df['FID']\n","\n","    target_column = f'EVI_{int(start_doy + (timeT_index-1)*interval)}'\n","    if target_column not in target_VI_df.columns:\n","        print(f\"Column '{target_column}' does not exist. Exiting function.\")\n","        return\n","\n","    sample_cluster_resultDF['EVI'] = target_VI_df[target_column]\n","\n","    clusters = refer_VI_curve_df['Cluster'].unique()\n","    for cluster in clusters:\n","        print('  cluster ',cluster,' labeling calculating ....')\n","        # get the refer curve information and values of given cluster\n","        refer_VI_curve_cluster = refer_VI_curve_df[refer_VI_curve_df['Cluster'] == cluster]\n","        # get refer surface reflectance harmonic parameters of given cluster\n","        refer_SRpara_cluster = refer_SRpara_df[refer_SRpara_df['Cluster'] == cluster]\n","\n","        # get the DTLS index value of given cluster without SAR\n","        dtw_distance_column,sad_distance_column = DTLS_singleCluster(refer_VI_curve_cluster,target_VI_df,target_lastT_srCurve,start_date,timeT,timeT_index,refer_SRpara_cluster,SR_bandnames,search_radius,interval)\n","\n","        # define the DTLS value column name of given cluster and add to the reslut dataframe\n","        dtw_distance_columnName = f'{int(cluster)}_dtw_distance'\n","        sad_distance_columnName = f'{int(cluster)}_sad_distance'\n","        sample_cluster_resultDF[dtw_distance_columnName] = dtw_distance_column#dtw_distance_column\n","        sample_cluster_resultDF[sad_distance_columnName] = sad_distance_column#sad_distance_column\n","\n","    sample_cluster_resultDF['cluster label'] = sample_cluster_resultDF[[distance_columnName for distance_columnName in sample_cluster_resultDF.columns if 'distance' in distance_columnName]].idxmin(axis=1)\n","    sample_cluster_resultDF['cluster label'] = sample_cluster_resultDF['cluster label'].str.replace('_distance', '', regex=False)\n","    sample_cluster_resultDF['latitude'] = target_VI_df['latitude']\n","    sample_cluster_resultDF['longitude'] = target_VI_df['longitude']\n","    # weite the final DTLS value of each sample with each cluster to a csv file\n","    sample_label_file = os.path.join(out_fileDir, f'sample_distance_{timeT}_{tilname}.csv')\n","    sample_cluster_resultDF.to_csv(sample_label_file, index=False)\n","\n","def DTLS_distance_combinate(Results_dir,VI_bandname,tilname,timeT_indexs):\n","    sample_label_file0_basename = next(f for f in os.listdir(Results_dir) if f.startswith('sample_distance_'))\n","    sample_label_file0 = os.path.join(Results_dir, sample_label_file0_basename)\n","    sample_label_df0 = pd.read_csv(sample_label_file0)\n","\n","    max_EVI = sample_label_df0.set_index('FID')[VI_bandname]  # 将 FID 设为索引\n","    max_sad_distances = sample_label_df0.set_index('FID')[[col for col in sample_label_df0.columns if '_sad_distance' in col]].copy()\n","    max_sad_distances = max_sad_distances/10\n","    cluster_names = set(col.split('_')[0] for col in sample_label_df0.columns if '_dtw_distance' in col)\n","\n","    for time_T in timeT_indexs:\n","        #time_T = int(time_T)\n","        sample_label_file = os.path.join(Results_dir, f'sample_distance_{time_T}_{tilname}.csv')\n","        if not os.path.exists(sample_label_file):\n","            print(f\"File {sample_label_file} does not exist. Skipping...\")\n","            continue\n","\n","        final_result_file = os.path.join(Results_dir, f'sample_label_{time_T}_{tilname}.csv')\n","        distance_df = pd.read_csv(sample_label_file).set_index('FID')\n","        final_distance_df = pd.DataFrame(index=distance_df.index)\n","\n","        EVI_cur = distance_df[VI_bandname]\n","        dtw_distance_curs = distance_df[[col for col in distance_df.columns if '_dtw_distance' in col]]\n","        sad_distance_curs = distance_df[[col for col in distance_df.columns if '_sad_distance' in col]]\n","        sad_distance_curs = sad_distance_curs/10\n","\n","        max_EVI = max_EVI.reindex(max_EVI.index.union(distance_df.index), fill_value=-np.inf)\n","        max_sad_distances = max_sad_distances.reindex(max_EVI.index)\n","\n","        merged_EVI = pd.concat([EVI_cur, max_EVI], axis=1, keys=['EVI_cur', 'EVI_max'])\n","\n","        for cluster in cluster_names:\n","            sad_distance_cur = sad_distance_curs[f'{cluster}_sad_distance']\n","            sad_distance_max = max_sad_distances[f'{cluster}_sad_distance']\n","\n","            # 更新 sad 距离：如果当前 EVI 大于最大 EVI，则用当前时相的 sad 距离，否则用最大 EVI 的 sad 距离\n","            updated_sad_distance = sad_distance_cur.where(\n","                (merged_EVI['EVI_cur'] > merged_EVI['EVI_max']) | merged_EVI['EVI_max'].isna(),\n","                sad_distance_max\n","            )\n","\n","            # 更新最大 EVI 和对应的 sad_distance\n","            max_EVI = merged_EVI['EVI_cur'].where(merged_EVI['EVI_cur'] > merged_EVI['EVI_max'], merged_EVI['EVI_max'])\n","            max_sad_distances[f'{cluster}_sad_distance'] = updated_sad_distance\n","\n","            # 计算 dtls 距离\n","            dtw_distance = dtw_distance_curs[f'{cluster}_dtw_distance']\n","            dtls_distance = dtw_distance + updated_sad_distance\n","            final_distance_df[f'{cluster}_distance'] = dtls_distance\n","\n","        # 获取最小距离的 cluster 作为标签\n","        final_distance_df['cluster label'] = final_distance_df[[col for col in final_distance_df.columns if 'distance' in col]].idxmin(axis=1)\n","        final_distance_df['cluster label'] = final_distance_df['cluster label'].str.replace('_distance', '', regex=False)\n","        final_distance_df['latitude'] = distance_df['latitude']\n","        final_distance_df['longitude'] = distance_df['longitude']\n","\n","        # 将 FID 添加回最终结果并保存\n","        final_distance_df.reset_index(inplace=True)  # 重置索引以便输出时包含 FID 列\n","        final_distance_df.to_csv(final_result_file, index=False)\n","        print(f'Time step {time_T} calculation done.')\n","\n","# main producer for generate training samples in target year\n","def generate_training_samples(target_features_fileDir, refer_SR_file, refer_VI_file, target_sampleResults_dir, tilename, refer_year, target_year):\n","    VI_bandname = 'EVI'\n","    SR_bandnames = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n","    search_radius = 2\n","    interval = 15\n","    start_date = datetime(target_year - 1, 10, 1)\n","    end_date = datetime(target_year, 7, 1)\n","\n","    timeT_indexs = pd.date_range(start=start_date, end=end_date, freq='15D')\n","    for timeT_index,timeT in enumerate(timeT_indexs,start=1):\n","        print('time ',timeT,'th labeling calculating....')\n","        DTLS_distance_calculate(target_features_fileDir,refer_SR_file,refer_VI_file,VI_bandname,SR_bandnames,target_sampleResults_dir,start_date,timeT,timeT_index,search_radius,tilename,interval,target_year)\n","        print('time ',timeT,'th labeling finished....')\n","    DTLS_distance_combinate(target_sampleResults_dir,VI_bandname,tilename,timeT_indexs)\n"],"metadata":{"id":"Q-Bm-XjWsHk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["refer_year = 2020\n","target_year = 2021\n","tilename = '30TYR'\n","target_features_fileDir = f'/content/drive/MyDrive/test_{tilename}/'\n","refer_SR_file = f'/content/drive/MyDrive/test_{tilename}/{tilename}_{refer_year}_SR_Reference_parameters.csv'\n","refer_VI_file = f'/content/drive/MyDrive/test_{tilename}/{tilename}_{refer_year}_VI_Reference_parameters.csv'\n","target_sampleResults_dir = f'/content/drive/MyDrive/test_{tilename}/target_samples/'\n","os.makedirs(target_sampleResults_dir, exist_ok=True)\n","generate_training_samples(target_features_fileDir, refer_SR_file, refer_VI_file, target_sampleResults_dir, tilename, refer_year, target_year)"],"metadata":{"id":"Zo9efpJ4sZyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Organize the label data into a single CSV file"],"metadata":{"id":"mg09cpZGxO86"}},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","\n","def extract_timeT_from_filename(filename, pattern_prefix, tilename):\n","    \"\"\"\n","    从文件名中提取时间字符串（如 sample_distance_2020-10-01 00:00:00_30TYR.csv → '2020-10-01'）\n","    \"\"\"\n","    pattern = rf\"{re.escape(pattern_prefix)}(\\d{{4}}-\\d{{2}}-\\d{{2}}) \\d{{2}}:\\d{{2}}:\\d{{2}}_{re.escape(tilename)}\\.csv\"\n","    match = re.search(pattern, filename)\n","    return match.group(1) if match else None  # 返回 '2020-10-01'\n","\n","def build_all_timewise_labels(results_dir, tilename, out_prefix='timewise'):\n","    dist_files = [f for f in os.listdir(results_dir) if f.startswith('sample_distance_') and f.endswith(f'{tilename}.csv')]\n","    label_files = [f for f in os.listdir(results_dir) if f.startswith('sample_label_version2') and f.endswith(f'{tilename}.csv')]\n","\n","    wide_from_dist = {}\n","    wide_from_label = {}\n","    latlon_dist = None\n","    latlon_label = None\n","\n","    # ------- Distance 文件 -------\n","    for f in dist_files:\n","        timeT = extract_timeT_from_filename(f, 'sample_distance_', tilename)\n","        if timeT is None:\n","            continue\n","        df = pd.read_csv(os.path.join(results_dir, f)).set_index('FID')\n","        if 'cluster label' not in df.columns:\n","            continue\n","        label = df['cluster label'].astype(str).str.extract(r'(\\d+)')[0].astype(int)\n","        label.name = f'T{timeT}'\n","        wide_from_dist[f'T{timeT}'] = label\n","        if latlon_dist is None and 'latitude' in df.columns and 'longitude' in df.columns:\n","            latlon_dist = df[['latitude', 'longitude']]\n","\n","    # ------- Label 文件 -------\n","    for f in label_files:\n","        timeT = extract_timeT_from_filename(f, 'sample_label_version2', tilename)\n","        if timeT is None:\n","            continue\n","        df = pd.read_csv(os.path.join(results_dir, f)).set_index('FID')\n","        if 'cluster label' not in df.columns:\n","            continue\n","        label = df['cluster label'].astype(int)\n","        label.name = f'T{timeT}'\n","        wide_from_label[f'T{timeT}'] = label\n","        if latlon_label is None and 'latitude' in df.columns and 'longitude' in df.columns:\n","            latlon_label = df[['latitude', 'longitude']]\n","\n","    # ------- 整合 Distance -------\n","    if wide_from_dist:\n","        dist_df = pd.concat(wide_from_dist.values(), axis=1)\n","        if latlon_dist is not None:\n","            dist_df = dist_df.join(latlon_dist, how='left')\n","        dist_df = dist_df.reset_index()\n","        time_cols = sorted([col for col in dist_df.columns if col.startswith('T')],\n","                           key=lambda x: x[1:])  # 排序按日期字符串\n","        dist_df = dist_df[['FID', 'latitude', 'longitude'] + time_cols]\n","        out_path = os.path.join(results_dir, f'{out_prefix}_labels_from_distance_{tilename}.csv')\n","        dist_df.to_csv(out_path, index=False)\n","        print(f'[saved] {out_path}')\n","    else:\n","        print('[warn] No distance-based label files found.')\n","\n","    # ------- 整合 Label Version -------\n","    if wide_from_label:\n","        label_df = pd.concat(wide_from_label.values(), axis=1)\n","        if latlon_label is not None:\n","            label_df = label_df.join(latlon_label, how='left')\n","        label_df = label_df.reset_index()\n","        time_cols = sorted([col for col in label_df.columns if col.startswith('T')],\n","                           key=lambda x: x[1:])  # 排序按日期字符串\n","        label_df = label_df[['FID', 'latitude', 'longitude'] + time_cols]\n","        out_path = os.path.join(results_dir, f'{out_prefix}_labels_from_labelFiles_{tilename}.csv')\n","        label_df.to_csv(out_path, index=False)\n","        print(f'[saved] {out_path}')\n","    else:\n","        print('[warn] No version2 label files found.')\n"],"metadata":{"id":"zREMSnaniJcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Results_dir = '/content/drive/MyDrive/test_30TYR/target_samples/'\n","tilname = '30TYR'\n","build_all_timewise_labels(results_dir=Results_dir, tilename=tilname)"],"metadata":{"id":"D1fKNJ9zxK9d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#4. multiple random forest classification (GEE)"],"metadata":{"id":"VHNGL9pybU48"}},{"cell_type":"markdown","source":["## upload the samples to asset for classification\n","take the time T April as example\n"],"metadata":{"id":"nLq_8D_5WZyn"}},{"cell_type":"code","source":["! pip install earthengine-api\n","import ee\n","ee.Authenticate()\n","ee.Initialize(project='crops-mapping-gaoyuan')\n","from ee.batch import Export\n","!pip install geemap\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import time\n","import numpy as np"],"metadata":{"id":"8u08TD0nfjJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def export_points_from_csv_balanced(\n","    csv_path,\n","    date_str,                                  # 例: '2020-04-14'\n","    out_asset_id,                              # 例: 'users/your_name/samples_30TYR_20200414'\n","    n_pos=1000, n_neg=1000,\n","    pos_label=1,\n","    prefer_neg_labels=(2,3,4,5,6),            # 优先覆盖的非1标签\n","    lon_col='longitude', lat_col='latitude', fid_col='FID',\n","    random_state=42\n","):\n","    # 1) 读取与列校验\n","    df = pd.read_csv(csv_path)\n","    label_col = f'T{date_str}'\n","    need_cols = [fid_col, lon_col, lat_col, label_col]\n","    for c in need_cols:\n","        if c not in df.columns:\n","            raise ValueError(f'CSV 缺少必要列: {c}')\n","\n","    # 基础清洗\n","    sub = df[need_cols].dropna(subset=[lon_col, lat_col, label_col]).copy()\n","    sub[lon_col] = pd.to_numeric(sub[lon_col], errors='coerce')\n","    sub[lat_col] = pd.to_numeric(sub[lat_col], errors='coerce')\n","    sub[label_col] = pd.to_numeric(sub[label_col], errors='coerce')\n","    sub = sub.dropna(subset=[lon_col, lat_col, label_col])\n","\n","    # 2) 分层抽样\n","    rng = np.random.default_rng(random_state)\n","\n","    # 正类\n","    pos_pool = sub[sub[label_col] == pos_label]\n","    if len(pos_pool) <= n_pos:\n","        pos_sample = pos_pool\n","    else:\n","        pos_sample = pos_pool.sample(n=n_pos, random_state=random_state)\n","\n","    # 非1总池\n","    neg_pool_all = sub[sub[label_col] != pos_label].copy()\n","\n","    # 先尽量覆盖 prefer_neg_labels = 2..6\n","    neg_samples = []\n","    present_pref_labels = [l for l in prefer_neg_labels if l in neg_pool_all[label_col].unique()]\n","\n","    # 目标为各类平均分配\n","    if present_pref_labels:\n","        base = n_neg // len(present_pref_labels)\n","        rem  = n_neg %  len(present_pref_labels)\n","\n","        # 每类目标配额\n","        target_per_class = {l: base for l in present_pref_labels}\n","        for l in present_pref_labels[:rem]:\n","            target_per_class[l] += 1\n","\n","        # 逐类抽样，不足则全取\n","        taken = 0\n","        leftover = 0\n","        for l in present_pref_labels:\n","            cls_df = neg_pool_all[neg_pool_all[label_col] == l]\n","            want = target_per_class[l]\n","            got = min(len(cls_df), want)\n","            if got > 0:\n","                neg_samples.append(cls_df.sample(n=got, random_state=random_state))\n","            taken += got\n","            leftover += max(0, want - got)\n","\n","        # 如果 2..6 不足，尝试用其它非1标签补齐\n","        if leftover > 0:\n","            others = neg_pool_all[~neg_pool_all[label_col].isin(present_pref_labels)]\n","            if len(others) > 0:\n","                need = min(leftover, len(others))\n","                neg_samples.append(others.sample(n=need, random_state=random_state))\n","    else:\n","        # 没有 2..6 的任何类时，直接从非1池中抽 n_neg\n","        need = min(n_neg, len(neg_pool_all))\n","        if need > 0:\n","            neg_samples.append(neg_pool_all.sample(n=need, random_state=random_state))\n","\n","    neg_sample = pd.concat(neg_samples, axis=0).drop_duplicates(subset=[fid_col]) if neg_samples else neg_pool_all.iloc[0:0]\n","    # 若仍不足 n_neg，尝试从剩余非1中再补\n","    if len(neg_sample) < n_neg:\n","        remain_pool = neg_pool_all.drop(neg_sample.index, errors='ignore')\n","        need = min(n_neg - len(neg_sample), len(remain_pool))\n","        if need > 0:\n","            neg_sample = pd.concat([neg_sample, remain_pool.sample(n=need, random_state=random_state)], axis=0)\n","\n","    # 截断到最多 n_neg\n","    if len(neg_sample) > n_neg:\n","        neg_sample = neg_sample.sample(n=n_neg, random_state=random_state)\n","\n","    # 汇总\n","    final_df = pd.concat([pos_sample, neg_sample], axis=0)\n","    # 打乱（可选）\n","    final_df = final_df.sample(frac=1.0, random_state=random_state)\n","\n","    print(f'正类(= {pos_label}) 可用 {len(pos_pool)}，抽取 {len(pos_sample)}')\n","    print(f'非{pos_label} 可用 {len(neg_pool_all)}，抽取 {len(neg_sample)}')\n","    print('最终样本量：', len(final_df))\n","\n","    # 3) 转为 ee.FeatureCollection\n","    feats = []\n","    for r in final_df.to_dict('records'):\n","        lon = float(r[lon_col]); lat = float(r[lat_col])\n","        lbl = int(r[label_col])\n","        fid = r[fid_col]\n","        try:\n","            fid = int(fid)\n","        except Exception:\n","            pass\n","        geom = ee.Geometry.Point([lon, lat])\n","        props = {\n","            'FID': fid,\n","            'label': lbl,\n","            'date': date_str,\n","            'longitude': lon,\n","            'latitude': lat\n","        }\n","        feats.append(ee.Feature(geom, props))\n","    fc = ee.FeatureCollection(feats)\n","\n","    # 4) 导出到 Asset\n","    desc = f'export_samples_balanced_{date_str.replace(\"-\",\"\")}'\n","    task = ee.batch.Export.table.toAsset(\n","        collection=fc,\n","        description=desc,\n","        assetId=out_asset_id\n","    )\n","    task.start()\n","    print(f'Started task: {desc}')\n","    print(f'Asset: {out_asset_id}')\n","    return fc\n","\n","# ===== 用法示例 =====\n","# 先在 Colab 挂载你的 Drive：\n","# from google.colab import drive\n","# drive.mount('/content')\n","\n","# 路径示例：你生成的“按日期为列”的宽表 CSV（label 或 distance 版本皆可）\n","csv_path = '/content/drive/MyDrive/test_30TYR/target_samples/timewise_labels_from_labelFiles_30TYR.csv'\n","date_str = '2021-04-14'\n","out_asset_id = 'projects/crops-mapping-gaoyuan/assets/samples_30TYR_20210414'  # 改成你的用户名路径\n","\n","_ = export_points_from_csv_balanced(\n","    csv_path, date_str, out_asset_id,\n","    n_pos=1000, n_neg=1000,\n","    pos_label=1, prefer_neg_labels=(2,3,4,5,6),\n","    random_state=2025\n",")"],"metadata":{"id":"aapGGRNVxYFk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## multiple Random Forest classification in GEE"],"metadata":{"id":"CmI1y5UaWolA"}},{"cell_type":"code","source":["# ---------- 1. 添加植被指数 ----------\n","def add_VIs(img):\n","    evi = img.expression(\n","        '2.5*(NIR - R)/(NIR + 6*R - 7.5*B + 1)',\n","        {\n","            'NIR': img.select('NIR'),\n","            'R': img.select('R'),\n","            'B': img.select('B')\n","        }\n","    ).rename('EVI')\n","\n","    lswi = img.normalizedDifference(['NIR', 'SWIR1']).rename('LSWI')\n","\n","    rep = img.select(['RE1', 'RE2', 'RE3']).expression(\n","        '((b(\"RE3\") + b(\"RE1\")) / 2) - b(\"RE2\")'\n","    ).rename('REP')\n","\n","    return img.addBands([evi, lswi, rep])\n","\n","# ---------- 2. 样本二分类 ----------\n","def relabel(f):\n","    label = ee.Number(f.get('label'))\n","    return f.set('code', label.eq(1))  # 1 为冬小麦，其它为 0\n","\n","# ---------- 3. 参数 ----------\n","year = 2021\n","interval = 15\n","tilename = '30TYR'\n","\n","s2_borderIndex = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/sentinel_2_index_shapefile\")\n","roi = s2_borderIndex.filter(ee.Filter.eq('Name', tilename)).geometry()\n","\n","samples = ee.FeatureCollection(\"projects/crops-mapping-gaoyuan/assets/samples_30TYR_20210414\").map(relabel)\n","\n","start_date = ee.Date.fromYMD(year - 1, 10, 1)\n","end_date = ee.Date.fromYMD(year, 10, 1)\n","\n","s2SR = get_s2sr_images(start_date, end_date, interval, roi)\n","s2SR = s2SR.map(add_VIs)\n","\n","sr_bands = ['B','G','R','RE1','RE2','RE3','NIR','SWIR1','SWIR2']\n","vi_bands = ['EVI', 'REP', 'LSWI']\n","all_bands = sr_bands + vi_bands\n","\n","# ---------- 4. 每个图像分类器 ----------\n","def classify_image(img):\n","    features = img.select(all_bands)\n","\n","    # 采样训练集\n","    training = features.sampleRegions(\n","        collection=samples,\n","        properties=['code'],\n","        scale=10,\n","        tileScale=4\n","    )\n","\n","    size = training.size()\n","\n","    # 有样本时分类，无样本时返回全掩膜图像，不计入平均\n","    classified = ee.Algorithms.If(\n","        size.gt(0),\n","        features.classify(\n","            ee.Classifier.smileRandomForest(100)\n","              .train(training, 'code', all_bands)\n","              .setOutputMode('PROBABILITY')\n","        ).multiply(100).toUint8()\n","         .updateMask(img.select('R').mask()),  # 保留原始掩膜\n","        ee.Image.constant(0).updateMask(ee.Image.constant(0))  # 完全掩膜，不影响平均\n","    )\n","\n","    return ee.Image(classified).set('mask', img.select('R').mask())\n","\n","# ---------- 5. 分类每个时相 ----------\n","classified_list = s2SR.map(classify_image)\n","\n","# ---------- 6. 融合所有时相 ----------\n","classified_ic = ee.ImageCollection(classified_list)\n","final_result = classified_ic.sum()\n","image_count  = classified_ic.count()\n","\n","final_prob = final_result.divide(image_count)\n","final_mask = final_prob.gte(50).selfMask()\n","\n","# ---------- 7. 导出 ----------\n","task = ee.batch.Export.image.toDrive(\n","    image=final_prob.clip(roi),\n","    description='export_wheat_prop_2021',\n","    folder='test_30TYR',\n","    fileNamePrefix='final_wheat_30TYR_prop_2021',\n","    region=roi.bounds(),\n","    scale=10,\n","    maxPixels=1e13,\n","    fileFormat='GeoTIFF'\n",")\n","task.start()\n","print('Export task started to Google Drive.')\n"],"metadata":{"id":"_BQeBsN1rHxM"},"execution_count":null,"outputs":[]}]}
